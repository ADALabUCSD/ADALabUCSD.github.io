# jemdoc: menu{MENU2}{cerebro.html}
= ADA Lab @ UCSD

~~~
{}{img_left}{images/cerebro.jpg}{}{}{80px}{}
== Project Cerebro
~~~ 

=== Overview

Artificial neural networks in the form of deep learning (DL) are revolutionizing 
many machine learning (ML) applications. Their success at major Web companies 
has created excitement among many enterprises and domain scientists to try DL 
for their applications. But training DL is a notoriously painful empirical process, 
since accuracy is tied to the data representation, neural architecture, and 
hyper-parameter settings. The common practice to choose these settings is to 
/empirically compare as many training configurations as feasible/ for the application, 
a central process in ML called /model selection/. This process is /inevitable/ 
because it is how one controls underfitting vs overfitting. 
Model selection is a major bottleneck for adoption of DL among enterprises and 
domain scientists due to both the /time spent/ and /resource costs/.

In this project, we are building a first-of-its-kind model selection-first platform 
for scalable DL that /raises model selection throughput without raising resource costs/. 
Our target setting is /small clusters/ (say, 10s of nodes), which covers a vast 
majority (over 90%) of parallel ML workloads in practice. 
We have 4 key system desiderata: /scalability/, /statistical convergence efficiency/, 
/reproducibility/, and /system generality/. 
To satisfy all these desiderata, we have been building a suite of novel parallel 
execution approaches to enable resource-efficient scalabilty at /all levels of 
the memory hierarchy/-clusters, cloud, local disks, DRAM, and GPU memory-and for 
/all axes of scalability/ in DL workloads: data sizes, model sizes, number of models
and tasks, number of data subgroups, and more.

Check out the CIDR 2021 paper below for an overview of our vision for Cerebro. 
Our techniques are inspired by classical lessons in RDBMS design and implementation
and operations research, including /multi-query optimization/, /materialized views/,
and /hybrid parallelism/. The technical papers cover our suite of new 
execution approaches, including /model hopper/ parallelism (MOP), 
/gradient accumulation/ parallelism (GAP), /shard alternation/ parallelism (SHARP), 
model fusion, and DL feature materialization.
All of our techniques are amenable to /non-disruptive integration/ with existing
DL frameworks such as TensorFlow and PyTorch, which makes practical adoption easier.

~~~
Cerebro is open sourced under Apache License v2.0. Code and deatiled documentation are available here: [https://adalabucsd.github.io/cerebro-system *Cerebro System*]
~~~

~~~
Most recent long talk on overall system vision: [https://www.youtube.com/watch?v=ERKyvLT4Wik *Youtube video*].
~~~

=== Overview Papers and Talks

- Some Damaging Delusions of Deep Learning Practice (and How to Avoid Them)\n
Arun Kumar, Supun Nakandala, and Yuhao Zhang\n
KDD 2021 Deep Learning Day | [papers/2021_DLDelusions_KDD.pdf Extended Abstract PDF] 
| [papers/2021_DLDelusions_KDD_Slides.pdf Talk slides]       
| [https://www.youtube.com/watch?v=UP9__WsfSuc Talk video]

- Cerebro: A Layered Data Platform for Scalable Deep Learning\n
Arun Kumar, Supun Nakandala, Yuhao Zhang, Side Li, Advitya Gemawat, and Kabir Nagrecha\n
CIDR 2021 (Vision paper) | [papers/2021_Cerebro_CIDR.pdf Paper PDF] and [papers/2021_Cerebro_CIDR.txt BibTeX] | [https://www.youtube.com/watch?v=8QfMvdlmdic Talk video]

- Cerebro: Efficient and Reproducible Model Selection on Deep Learning Systems\n
Supun Nakandala, Yuhao Zhang, and Arun Kumar\n
ACM SIGMOD 2019 DEEM Workshop | [papers/2019_Cerebro_DEEM.pdf Paper PDF] and [papers/2019_Cerebro_DEEM.txt BibTeX] | [https://adalabucsd.github.io/research-blog/cerebro.html Blog post]

=== Transfer Learning

- Nautilus: An Optimized System for Deep Transfer Learning over Evolving Training Datasets\n
Supun Nakandala and Arun Kumar\n
SIGMOD 2022 | [papers/2022_Nautilus_SIGMOD.pdf Paper PDF] | [papers/TR_2021_Nautilus.pdf TechReport] | Code release coming soon

- Also see [https://adalabucsd.github.io/vista.html Project Vista].

=== Model Scalability

- Hydra: A Data System for Large Multi-Model Deep Learning\n
Kabir Nagrecha and Arun Kumar\n
Under submission | [https://arxiv.org/abs/2110.08633 TechReport]

- Model Parallel Model Selection for Deep Learning Systems\n
Kabir Nagrecha\n
SIGMOD 2021 (Student Research Competition Winner) | [https://arxiv.org/abs/2107.06469 Arxiv]

=== Data Scalability

- Distributed Deep Learning on Data Systems: A Comparative Analysis of Approaches\n
Yuhao Zhang, Frank McQuillan, Nandish Jayaram, Nikhil Kak, Ekta Khanna, Orhan Kislal, Domino Valdano, and Arun Kumar\n
VLDB 2021 | [papers/2021_Cerebro-DS.pdf Paper PDF] | [papers/TR_2021_Cerebro-DS.pdf TechReport] | [https://youtu.be/SK9wTzO4K7M Talk video] | [https://github.com/makemebitter/cerebro-ds/ Code release]

- Cerebro: A Data System for Optimized Deep Learning Model Selection\n
Supun Nakandala, Yuhao Zhang, and Arun Kumar\n
VLDB 2020 | [papers/2020_Cerebro_VLDB.pdf Paper PDF] and [papers/2020_Cerebro_VLDB.txt BibTeX] | [papers/2020_Cerebro_VLDB_Errata.pdf Errata] | [papers/TR_2020_Cerebro.pdf TechReport]
 | Talk videos: [https://www.youtube.com/watch?v=8PJic5FStGs Youtube] [https://www.bilibili.com/video/av329339128?p=198 Bilibili]
 | [https://adalabucsd.github.io/research-blog/cerebro.html Blog post] | [https://databricks.com/session_na20/resource-efficient-deep-learning-model-selection-on-apache-spark SAIS Talk video]
| [https://adalabucsd.github.io/cerebro-system/ Source code and documentation]

=== New Abstractions and Interfaces

- Intermittent Human-in-the-Loop Model Selection using Cerebro: A Demonstration\n
Liangde Li, Supun Nakandala, and Arun Kumar\n
VLDB 2021 Demo | [papers/2021_Cerebro_VLDB_Demo.pdf Paper PDF] | [papers/TR_2021_Intermittent_HIL_MS.pdf TechReport] | [https://youtu.be/K3THQy5McXc Video]

- Towards an Optimized GROUP BY Abstraction for Large-Scale Machine Learning\n
Side Li and Arun Kumar\n
VLDB 2021 | [papers/2021_Kingpin_VLDB.pdf Paper PDF] | [papers/TR_2021_Kingpin.pdf TechReport] | [https://www.youtube.com/watch?v=OlTknBfBmvM Talk video] | [https://github.com/liside/Kingpin Code Release]

=== Applications

- The CNN Hip Accelerometer Posture (CHAP) Method for Classifying Sitting Patterns from Hip Accelerometers: A Validation Study\n
Mikael Anne Greenwood-Hickman, Supun Nakandala, Marta M. Jankowska, Fatima Tuz-Zahra, John Bellettiere, Jordan Carlson, Paul R. Hibbing, Jingjing Zou, Andrea Z. LaCroix, Arun Kumar, and Loki Natarajan\n
Medicine and Science in Sports and Exercise Journal, 2021 | [papers/2021_MSSE_CHAP.pdf Paper PDF] | [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8516667/ On PubMed] | [https://github.com/ADALabUCSD/DeepPostures Code]

- Application of Convolutional Neural Network Algorithms for Advancing Sedentary and Activity Bout Classification\n
Supun Nakandala, Marta Jankowska, Fatima Tuz-Zahra, John Bellettiere, Jordan Carlson, Andrea LaCroix, Sheri Hartman, Dori Rosenberg, Jingjing Zou, Arun Kumar, and Loki Natarajan\n
Journal for the Measurement of Physical Behaviour, 2021 | [papers/2021_JMPB_CNN.pdf Paper PDF] and [papers/2021_JMPB_CNN.txt BibTeX] | [https://github.com/ADALabUCSD/DeepPostures Code]

=== Student Contact

- Supun Nakandala: snakanda \[at\] eng \[dot\] ucsd \[dot\] edu
- Yuhao Zhang: yuz870 \[at\] eng \[dot\] ucsd \[dot\] edu
- Kabir Nagrecha: knagrech \[at\] ucsd \[dot\] edu
- Pradyumna Sridhara: prsridha \[at\] ucsd \[dot\] edu
- Vignesh Nanda Kumar: vnandakumar \[at\] ucsd \[dot\] edu

=== Acknowledgments

This project was\/is supported in part by a Hellman Fellowship, the NIDDK of the NIH under award number R01DK114945, an NSF CAREER Award under award number 1942724, and gifts from VMware.
